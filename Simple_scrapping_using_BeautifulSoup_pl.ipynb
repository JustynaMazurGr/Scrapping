{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRz4DxLqySeU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nb9-Z9f1yWA8"
      },
      "outputs": [],
      "source": [
        "base_page_url = \"https://www.somejobpage?pn=\"\n",
        "page = requests.get(base_page_url)\n",
        "soup = BeautifulSoup(page.content, 'html.parser')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUUPg3zNoi9e",
        "outputId": "e75bbf92-587c-4b39-eecc-13ba806ca9c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1019\n"
          ]
        }
      ],
      "source": [
        "max_page_element = soup.find('span', {'data-test': 'top-pagination-max-page-number'})\n",
        "max_pages = int(max_page_element.text.strip())\n",
        "print(max_pages)  #To check how many pages there are to scrape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_page_url = \"https://www.somejobpage\"\n",
        "\n",
        "# Superclass\n",
        "class Page_And_Soup():\n",
        "    def __init__(self, page_url):\n",
        "        self.page = requests.get(page_url)\n",
        "        self.soup = BeautifulSoup(self.page.content, 'html.parser')\n",
        "\n",
        "# Subclass\n",
        "class Scrapping(Page_And_Soup):\n",
        "    def __init__(self, page_url):\n",
        "        # Initialisation of base class attributes\n",
        "        super().__init__(page_url)\n",
        "\n",
        "        # Creating a list of headers for the new DataFrame\n",
        "        self.head = [\"Title\", \"Link\", \"Company\", \"Region\", \"Additional_Info\", \"Current_page_url\"]\n",
        "\n",
        "        # Adding subsequent data from advertisements to a list\n",
        "        self.Title = []\n",
        "        self.Link = []\n",
        "        self.Company = []\n",
        "        self.Region = []\n",
        "        self.Additional_Info = []\n",
        "        self.Current_page_url = []\n",
        "\n",
        "        # Extracting data for each job offer\n",
        "        self.job_offers = self.soup.find_all('div', class_='tiles_c15wfuie')\n",
        "\n",
        "        for offer in self.job_offers:\n",
        "            self.offer_title = offer.find('h2', class_='tiles_h1yuv00i').text.strip()\n",
        "            self.Title.append(self.offer_title)\n",
        "\n",
        "            self.offer_link = offer.find('a', class_='core_n194fgoq')['href']\n",
        "            self.Link.append(self.offer_link)\n",
        "\n",
        "            self.company_name = offer.find('div', class_='tiles_ctlhhld').text.strip()\n",
        "            self.Company.append(self.company_name)\n",
        "\n",
        "            if offer.find('h4', class_='tiles_r1h1nge7'):\n",
        "                self.region = offer.find('h4', class_='tiles_r1h1nge7').text\n",
        "                self.Region.append(self.region)\n",
        "\n",
        "            self.additional_info = []\n",
        "            for info_item in offer.find_all('li', class_='tiles_iwlrcdk'):\n",
        "                self.additional_info.append(info_item.text.strip())\n",
        "            self.Additional_Info.append(self.additional_info)\n",
        "\n",
        "            self.Current_page_url.append(page_url)\n",
        "\n",
        "# Retrieving data for page 1\n",
        "page_1 = Scrapping(base_page_url)\n",
        "\n",
        "# Creating DataFrame from the first page\n",
        "df = pd.DataFrame(list(zip(page_1.Title, page_1.Link, page_1.Company, page_1.Region, page_1.Additional_Info, page_1.Current_page_url)),\n",
        "                  columns=page_1.head)\n",
        "\n",
        "# Code for pages 2 to max_pages\n",
        "for current_number in range(2, max_pages + 1):\n",
        "    next_page_url = f\"{base_page_url}?pn={current_number}\"  # It loops through and thus I get the addresses of successive pages\n",
        "    next_pages = Scrapping(next_page_url)  # Scraping successive pages\n",
        "\n",
        "    # Collect data in lists\n",
        "    data_to_concat = list(\n",
        "        zip(next_pages.Title, next_pages.Link, next_pages.Company, next_pages.Region, next_pages.Additional_Info,\n",
        "            next_pages.Current_page_url))\n",
        "\n",
        "# Converting data to DataFrame\n",
        "new_data_df = pd.DataFrame(data_to_concat, columns=page_1.head)\n",
        "\n",
        "# Append data to the DataFrame\n",
        "df = pd.concat([df, new_data_df], ignore_index=True)\n",
        "\n",
        "# Saving to file\n",
        "df.to_csv(\"/content/drive/MyDrive/job_offers.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "tmV2F6e9bxrW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}